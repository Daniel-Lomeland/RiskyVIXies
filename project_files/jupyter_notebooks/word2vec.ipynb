{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "import pprint\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = '../../../../../APIKeys/API.yaml'\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as config_file:\n",
    "    config = yaml.load(config_file)\n",
    "\n",
    "PLOTLY_USERNAME = config[\"plotly\"][\"PLOTLY_USERNAME\"]\n",
    "PLOTLY_API_KEY = config[\"plotly\"][\"PLOTLY_API_KEY\"]\n",
    "\n",
    "plotly.tools.set_credentials_file(username = PLOTLY_USERNAME, api_key = PLOTLY_API_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Published</th>\n",
       "      <th>Snippet</th>\n",
       "      <th>Lead Paragraph</th>\n",
       "      <th>Main Headline</th>\n",
       "      <th>Print Headline</th>\n",
       "      <th>Seo Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>2019-01-01T07:30:05+0000</td>\n",
       "      <td>Underperforming media deals have left the conf...</td>\n",
       "      <td>Underperforming media deals have left the conf...</td>\n",
       "      <td>Another Season Comes and Goes While Pac-12 Str...</td>\n",
       "      <td>A Power Conference’s Declining Clout</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>2019-01-01T14:00:04+0000</td>\n",
       "      <td>Recent commercial real estate transactions in ...</td>\n",
       "      <td>Recent commercial real estate transactions in ...</td>\n",
       "      <td>Recent Commercial Real Estate Transactions</td>\n",
       "      <td>Transactions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2019-01-01T17:47:14+0000</td>\n",
       "      <td>Skeptical reporting has still been too favorable.</td>\n",
       "      <td>Skeptical reporting has still been too favorable.</td>\n",
       "      <td>The Trump Tax Cut: Even Worse Than You’ve Heard</td>\n",
       "      <td>The Trump Tax Cut: Even Worse Than You’ve Heard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>2019-01-01T21:21:40+0000</td>\n",
       "      <td>The episode of “Patriot Act With Hasan Minhaj,...</td>\n",
       "      <td>The episode of “Patriot Act With Hasan Minhaj,...</td>\n",
       "      <td>Netflix Blocks Show in Saudi Arabia Critical o...</td>\n",
       "      <td>Netflix Blocks Show in Saudi Arabia Critical o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>2019-01-02T07:00:00+0000</td>\n",
       "      <td>This word has appeared in 10 articles on NYTim...</td>\n",
       "      <td>This word has appeared in 10 articles on NYTim...</td>\n",
       "      <td>Word + Quiz: agog</td>\n",
       "      <td>Word + Quiz: agog</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Published  \\\n",
       "977  2019-01-01T07:30:05+0000   \n",
       "674  2019-01-01T14:00:04+0000   \n",
       "879  2019-01-01T17:47:14+0000   \n",
       "307  2019-01-01T21:21:40+0000   \n",
       "357  2019-01-02T07:00:00+0000   \n",
       "\n",
       "                                               Snippet  \\\n",
       "977  Underperforming media deals have left the conf...   \n",
       "674  Recent commercial real estate transactions in ...   \n",
       "879  Skeptical reporting has still been too favorable.   \n",
       "307  The episode of “Patriot Act With Hasan Minhaj,...   \n",
       "357  This word has appeared in 10 articles on NYTim...   \n",
       "\n",
       "                                        Lead Paragraph  \\\n",
       "977  Underperforming media deals have left the conf...   \n",
       "674  Recent commercial real estate transactions in ...   \n",
       "879  Skeptical reporting has still been too favorable.   \n",
       "307  The episode of “Patriot Act With Hasan Minhaj,...   \n",
       "357  This word has appeared in 10 articles on NYTim...   \n",
       "\n",
       "                                         Main Headline  \\\n",
       "977  Another Season Comes and Goes While Pac-12 Str...   \n",
       "674         Recent Commercial Real Estate Transactions   \n",
       "879    The Trump Tax Cut: Even Worse Than You’ve Heard   \n",
       "307  Netflix Blocks Show in Saudi Arabia Critical o...   \n",
       "357                                  Word + Quiz: agog   \n",
       "\n",
       "                                        Print Headline  Seo Headline  \n",
       "977               A Power Conference’s Declining Clout           NaN  \n",
       "674                                       Transactions           NaN  \n",
       "879    The Trump Tax Cut: Even Worse Than You’ve Heard           NaN  \n",
       "307  Netflix Blocks Show in Saudi Arabia Critical o...           NaN  \n",
       "357                                  Word + Quiz: agog           NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_data = pd.read_csv('../../business_news.csv')\n",
    "# main_data = pd.read_csv('../../entrepreneurs_news.csv')\n",
    "# main_data = pd.read_csv('../../financial_news.csv')\n",
    "\n",
    "main_data = main_data.sort_values(by = \"Published\")\n",
    "\n",
    "news_data = main_data[[\"Snippet\", \"Lead Paragraph\", \"Main Headline\"]]\n",
    "news_data.apply(str)\n",
    "main_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Underperforming media deals have left the conference falling behind its peers off the field. That might be the root of its problems on it.', 'Underperforming media deals have left the conference falling behind its peers off the field. That might be the root of its problems on it.', 'Another Season Comes and Goes While Pac-12 Struggles to Keep Up']\n"
     ]
    }
   ],
   "source": [
    "def list_prep(rows_to_list):\n",
    "    news_list = rows_to_list.values.tolist()\n",
    "    return(news_list)\n",
    "    \n",
    "list_of_lists = list_prep(news_data)\n",
    "print(list_of_lists[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['underperforming', 'media', 'deals', 'left', 'conference', 'falling', 'behind', 'peers', 'field', 'might', 'root', 'problems', 'underperforming', 'media', 'deals', 'left', 'conference', 'falling', 'behind', 'peers', 'field', 'might', 'root', 'problems', 'another', 'season', 'comes', 'goes', 'struggles', 'keep']\n"
     ]
    }
   ],
   "source": [
    "def word_prep(list_lists):\n",
    "    big_string = \"\"\n",
    "\n",
    "    for title in list_lists:\n",
    "    #     Splitting titles into list of words\n",
    "        variable = title.split(\" \")\n",
    "    #     print(variable)\n",
    "\n",
    "    #     v = words\n",
    "    #     variable = list of words\n",
    "        for v in variable:\n",
    "            big_string = big_string + \" \" + v\n",
    "\n",
    "    #     big_title_string = ' '.join((str(v) for v in variable))\n",
    "\n",
    "    tokens = word_tokenize(big_string)\n",
    "\n",
    "    # Remove non-alphabetic tokens, such as punctuation\n",
    "    words = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "\n",
    "    # Print first 10 words\n",
    "    return(words)\n",
    "\n",
    "\n",
    "    # print(tokens)\n",
    "word_list = []\n",
    "for list in list_of_lists:\n",
    "    words = word_prep(list)\n",
    "    word_list.append(words)\n",
    "print(word_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load word2vec model (trained on an enormous Google corpus)\n",
    "model = gensim.models.KeyedVectors.\\\n",
    "load_word2vec_format('../resources/model/GoogleNews-vectors-negative300.bin', binary = True) \n",
    "\n",
    "# Check dimension of word vectors\n",
    "model.vector_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_2_vec(words):\n",
    "    vector_list = []\n",
    "    words_filtered = []\n",
    "    # Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "    vector_list.extend([model[word] for word in words if word in model.vocab])\n",
    "#     return(vector_list)\n",
    "    # Create a list of the words corresponding to these vectors\n",
    "    words_filtered.extend([word for word in words if word in model.vocab])\n",
    "#     return(words_filtered)\n",
    "    # Zip the words together with their vector representations\n",
    "    word_vec_zip = zip(words_filtered, vector_list)\n",
    "    \n",
    "    # Cast to a dict so we can turn it into a dataframe\n",
    "    word_vec_dict = dict(word_vec_zip)\n",
    "    return(word_vec_dict)\n",
    "    \n",
    "\n",
    "\n",
    "dataframe_dict_list = []\n",
    "for list in word_list:\n",
    "    df = word_2_vec(list)\n",
    "#     print(z)\n",
    "    dataframe_dict_list.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>underperforming</th>\n",
       "      <th>media</th>\n",
       "      <th>deals</th>\n",
       "      <th>left</th>\n",
       "      <th>conference</th>\n",
       "      <th>falling</th>\n",
       "      <th>behind</th>\n",
       "      <th>peers</th>\n",
       "      <th>field</th>\n",
       "      <th>might</th>\n",
       "      <th>root</th>\n",
       "      <th>problems</th>\n",
       "      <th>another</th>\n",
       "      <th>season</th>\n",
       "      <th>comes</th>\n",
       "      <th>goes</th>\n",
       "      <th>struggles</th>\n",
       "      <th>keep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.166992</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.105957</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>0.125977</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>-0.294922</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>-0.116211</td>\n",
       "      <td>-0.023560</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.059814</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.090332</td>\n",
       "      <td>0.378906</td>\n",
       "      <td>0.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.238281</td>\n",
       "      <td>-0.009277</td>\n",
       "      <td>0.337891</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.241211</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>0.243164</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>-0.019653</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>-0.012939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.291016</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>-0.088379</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>-0.116211</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.123047</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>-0.114258</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>-0.159180</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>-0.061768</td>\n",
       "      <td>-0.208008</td>\n",
       "      <td>-0.108887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.107422</td>\n",
       "      <td>-0.281250</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>-0.076660</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>-0.158203</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>0.324219</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.104492</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>0.059082</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.143555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.135742</td>\n",
       "      <td>0.016113</td>\n",
       "      <td>0.028809</td>\n",
       "      <td>-0.055664</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>-0.033691</td>\n",
       "      <td>-0.106445</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>0.034668</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>-0.527344</td>\n",
       "      <td>-0.236328</td>\n",
       "      <td>-0.071777</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>-0.083984</td>\n",
       "      <td>-0.013184</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>-0.087402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   underperforming     media     deals      left  conference   falling  \\\n",
       "0        -0.166992  0.097656  0.105957  0.233398   -0.146484 -0.085449   \n",
       "1         0.238281 -0.009277  0.337891  0.073242    0.118164  0.241211   \n",
       "2         0.291016 -0.267578 -0.088379  0.044434    0.242188 -0.116211   \n",
       "3        -0.107422 -0.281250 -0.022827 -0.010254   -0.076660 -0.001678   \n",
       "4        -0.135742  0.016113  0.028809 -0.055664    0.062988 -0.033691   \n",
       "\n",
       "     behind     peers     field     might      root  problems   another  \\\n",
       "0  0.125977 -0.275391 -0.294922  0.227539 -0.116211 -0.023560  0.194336   \n",
       "1 -0.014709  0.074707  0.146484  0.120117  0.243164  0.163086 -0.019653   \n",
       "2  0.145508 -0.164062  0.123047  0.068359  0.163086 -0.114258  0.091797   \n",
       "3 -0.158203  0.351562 -0.292969  0.324219  0.130859  0.273438  0.104492   \n",
       "4 -0.106445  0.134766  0.034668 -0.161133 -0.527344 -0.236328 -0.071777   \n",
       "\n",
       "     season     comes      goes  struggles      keep  \n",
       "0  0.059814  0.265625  0.090332   0.378906  0.060547  \n",
       "1  0.164062  0.037842  0.148438   0.289062 -0.012939  \n",
       "2 -0.159180  0.069336 -0.061768  -0.208008 -0.108887  \n",
       "3  0.061523 -0.029419  0.059082   0.132812  0.143555  \n",
       "4  0.197266 -0.083984 -0.013184   0.004395 -0.087402  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataframe_list = []\n",
    "for dataframe in range(len(dataframe_dict_list)):\n",
    "    df = pd.DataFrame.from_dict(dataframe_dict_list[dataframe])\n",
    "    dataframe_list.append(df)\n",
    "# df.shape\n",
    "dataframe_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.040616\n",
      "1    0.129954\n",
      "2   -0.002753\n",
      "3    0.033381\n",
      "4   -0.057427\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "def row_summation(dataframe_list):\n",
    "    summation_list = []\n",
    "    for dataframe in dataframe_list:\n",
    "    #     print(dataframe)\n",
    "        total_columns = len(dataframe.columns)\n",
    "    #     print(total_columns)\n",
    "        sums = dataframe.sum(axis = 1, skipna = True)\n",
    "    #     print(sums)\n",
    "        summation = sums/total_columns\n",
    "        summation_list.append(summation)\n",
    "    return summation_list\n",
    "\n",
    "    \n",
    "summation_series = row_summation(dataframe_list)\n",
    "print(summation_series[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_list = main_data['Published'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01T07:30:05+0000</th>\n",
       "      <td>0.040616</td>\n",
       "      <td>0.129954</td>\n",
       "      <td>-0.002753</td>\n",
       "      <td>0.033381</td>\n",
       "      <td>-0.057427</td>\n",
       "      <td>-0.013980</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>-0.091292</td>\n",
       "      <td>0.117317</td>\n",
       "      <td>0.057339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050925</td>\n",
       "      <td>0.051815</td>\n",
       "      <td>-0.130132</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>-0.025178</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.025302</td>\n",
       "      <td>-0.020338</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>0.002306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01T14:00:04+0000</th>\n",
       "      <td>-0.006548</td>\n",
       "      <td>0.075945</td>\n",
       "      <td>-0.091047</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>0.058803</td>\n",
       "      <td>-0.076625</td>\n",
       "      <td>-0.049800</td>\n",
       "      <td>-0.061550</td>\n",
       "      <td>0.137486</td>\n",
       "      <td>0.082084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.019948</td>\n",
       "      <td>-0.003523</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>-0.006598</td>\n",
       "      <td>-0.036187</td>\n",
       "      <td>-0.053292</td>\n",
       "      <td>-0.040545</td>\n",
       "      <td>-0.105887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01T17:47:14+0000</th>\n",
       "      <td>0.051382</td>\n",
       "      <td>0.038457</td>\n",
       "      <td>-0.040468</td>\n",
       "      <td>0.091321</td>\n",
       "      <td>-0.105566</td>\n",
       "      <td>0.050043</td>\n",
       "      <td>0.067348</td>\n",
       "      <td>0.034418</td>\n",
       "      <td>0.221899</td>\n",
       "      <td>0.109424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018445</td>\n",
       "      <td>0.025484</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>-0.013770</td>\n",
       "      <td>-0.054761</td>\n",
       "      <td>-0.026877</td>\n",
       "      <td>-0.051270</td>\n",
       "      <td>0.094012</td>\n",
       "      <td>-0.003528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01T21:21:40+0000</th>\n",
       "      <td>-0.027873</td>\n",
       "      <td>0.047825</td>\n",
       "      <td>0.052103</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>-0.090712</td>\n",
       "      <td>0.011233</td>\n",
       "      <td>0.030041</td>\n",
       "      <td>-0.041097</td>\n",
       "      <td>0.083566</td>\n",
       "      <td>0.067819</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020653</td>\n",
       "      <td>-0.104341</td>\n",
       "      <td>-0.046093</td>\n",
       "      <td>-0.068028</td>\n",
       "      <td>-0.011126</td>\n",
       "      <td>0.021588</td>\n",
       "      <td>-0.013541</td>\n",
       "      <td>-0.066595</td>\n",
       "      <td>0.059064</td>\n",
       "      <td>0.035169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02T07:00:00+0000</th>\n",
       "      <td>0.144113</td>\n",
       "      <td>0.129674</td>\n",
       "      <td>-0.027830</td>\n",
       "      <td>0.098580</td>\n",
       "      <td>-0.037144</td>\n",
       "      <td>-0.040632</td>\n",
       "      <td>0.109305</td>\n",
       "      <td>-0.168248</td>\n",
       "      <td>0.135254</td>\n",
       "      <td>0.042855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019758</td>\n",
       "      <td>-0.118478</td>\n",
       "      <td>-0.038942</td>\n",
       "      <td>-0.057478</td>\n",
       "      <td>0.082912</td>\n",
       "      <td>-0.200335</td>\n",
       "      <td>-0.007311</td>\n",
       "      <td>-0.116525</td>\n",
       "      <td>-0.052213</td>\n",
       "      <td>-0.024972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0         1         2         3         4    \\\n",
       "2019-01-01T07:30:05+0000  0.040616  0.129954 -0.002753  0.033381 -0.057427   \n",
       "2019-01-01T14:00:04+0000 -0.006548  0.075945 -0.091047  0.003771  0.058803   \n",
       "2019-01-01T17:47:14+0000  0.051382  0.038457 -0.040468  0.091321 -0.105566   \n",
       "2019-01-01T21:21:40+0000 -0.027873  0.047825  0.052103  0.020469 -0.090712   \n",
       "2019-01-02T07:00:00+0000  0.144113  0.129674 -0.027830  0.098580 -0.037144   \n",
       "\n",
       "                               5         6         7         8         9    \\\n",
       "2019-01-01T07:30:05+0000 -0.013980  0.042677 -0.091292  0.117317  0.057339   \n",
       "2019-01-01T14:00:04+0000 -0.076625 -0.049800 -0.061550  0.137486  0.082084   \n",
       "2019-01-01T17:47:14+0000  0.050043  0.067348  0.034418  0.221899  0.109424   \n",
       "2019-01-01T21:21:40+0000  0.011233  0.030041 -0.041097  0.083566  0.067819   \n",
       "2019-01-02T07:00:00+0000 -0.040632  0.109305 -0.168248  0.135254  0.042855   \n",
       "\n",
       "                            ...          290       291       292       293  \\\n",
       "2019-01-01T07:30:05+0000    ...    -0.050925  0.051815 -0.130132  0.007858   \n",
       "2019-01-01T14:00:04+0000    ...    -0.053502  0.019948 -0.003523  0.008057   \n",
       "2019-01-01T17:47:14+0000    ...     0.018445  0.025484  0.005743  0.020642   \n",
       "2019-01-01T21:21:40+0000    ...    -0.020653 -0.104341 -0.046093 -0.068028   \n",
       "2019-01-02T07:00:00+0000    ...     0.019758 -0.118478 -0.038942 -0.057478   \n",
       "\n",
       "                               294       295       296       297       298  \\\n",
       "2019-01-01T07:30:05+0000 -0.025178  0.005754  0.025302 -0.020338  0.020962   \n",
       "2019-01-01T14:00:04+0000  0.019845 -0.006598 -0.036187 -0.053292 -0.040545   \n",
       "2019-01-01T17:47:14+0000 -0.013770 -0.054761 -0.026877 -0.051270  0.094012   \n",
       "2019-01-01T21:21:40+0000 -0.011126  0.021588 -0.013541 -0.066595  0.059064   \n",
       "2019-01-02T07:00:00+0000  0.082912 -0.200335 -0.007311 -0.116525 -0.052213   \n",
       "\n",
       "                               299  \n",
       "2019-01-01T07:30:05+0000  0.002306  \n",
       "2019-01-01T14:00:04+0000 -0.105887  \n",
       "2019-01-01T17:47:14+0000 -0.003528  \n",
       "2019-01-01T21:21:40+0000  0.035169  \n",
       "2019-01-02T07:00:00+0000 -0.024972  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summation_dict = {}\n",
    "published_index = 0\n",
    "for series in summation_series:\n",
    "    summation_dict.update({published_list[published_index]: series})\n",
    "    published_index += 1\n",
    "    \n",
    "summation_df = pd.DataFrame(summation_dict)\n",
    "summation_df = summation_df.T\n",
    "summation_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "summation_df.to_csv(\"../resources/news/financial_news_word2vec.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "# Use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(df[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "# Second plot: Import adjustText, initialize list of texts\n",
    "from adjustText import adjust_text\n",
    "texts = []\n",
    "words_to_plot = list(np.arange(0, 400, 10))\n",
    "\n",
    "# Append words to list\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
    "    \n",
    "# Plot text using adjust_text (because overlapping text is hard to read)\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = {\n",
    "    'data': [\n",
    "  \t\t{\n",
    "  \t\t\t'x': tsne_df[:, 0], \n",
    "        \t'y': tsne_df[:, 1], \n",
    "        \t'text': df.index,\n",
    "            \"mode\": \"markers\"\n",
    "        }]}   \n",
    "\n",
    "py.plot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.vocab]\n",
    "    return np.mean(model[doc], axis=0)\n",
    "\n",
    "# Our earlier preprocessing was done when we were dealing only with word vectors\n",
    "# Here, we need each document to remain a document \n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] \n",
    "    return doc\n",
    "\n",
    "# Function that will help us drop documents that have no word vectors in word2vec\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)\n",
    "\n",
    "# Filter out documents\n",
    "def filter_docs(corpus, texts, condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    if texts is not None:\n",
    "        texts = [text for (text, doc) in zip(texts, corpus)\n",
    "                 if condition_on_doc(doc)]\n",
    "\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the corpus\n",
    "corpus = [preprocess(title) for title in titles_list]\n",
    "\n",
    "# Remove docs that don't include any words in W2V's vocab\n",
    "corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: has_vector_representation(model, doc))\n",
    "\n",
    "# Filter out any empty docs\n",
    "corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: (len(doc) != 0))\n",
    "x = []\n",
    "for doc in corpus: # append the vector for each document\n",
    "    x.append(document_vector(model, doc))\n",
    "    \n",
    "X = np.array(x) # list to array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "# Again use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(X[:400])\n",
    "fig, ax = plt.subplots(figsize = (14, 10))\n",
    "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "texts = []\n",
    "titles_to_plot = list(np.arange(0, 400, 40)) # plots every 40th title in first 400 titles\n",
    "\n",
    "# Append words to list\n",
    "for title in titles_to_plot:\n",
    "    texts.append(plt.text(tsne_df[title, 0], tsne_df[title, 1], titles_list[title], fontsize = 14))\n",
    "    \n",
    "# Plot text using adjust_text\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titles_to_plot)\n",
    "type(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = {\n",
    "    'data': [\n",
    "  \t\t{\n",
    "  \t\t\t'x': tsne_df[:, 0], \n",
    "        \t'y': tsne_df[:, 1], \n",
    "        \t'text': titles_list[title],\n",
    "            \"mode\": \"markers\"\n",
    "        }]}   \n",
    "\n",
    "py.plot(fig2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
